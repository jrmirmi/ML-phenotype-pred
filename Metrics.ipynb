{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVtkQEN/K3vVdDJUF0ELTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrmirmi/ML-phenotype-pred/blob/main/Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN THIS FILE THERE ARE FUNCTIONS FOR EVALUATION METRICS AND PLOT\n"
      ],
      "metadata": {
        "id": "EaPsQQvVu3ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "classification_report, \n",
        "confusion_matrix, \n",
        "accuracy_score, \n",
        "balanced_accuracy_score, \n",
        "precision_score, recall_score, \n",
        "f1_score, roc_curve, \n",
        "RocCurveDisplay, \n",
        "auc)\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "etIshLHGNjg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for compute the specificity\n",
        "def specificity_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape != (2, 2):\n",
        "        raise ValueError('The problem must be binary for specificity')\n",
        "    specificity = cm[0][0] / (cm[0][0] + cm[0][1])\n",
        "    return specificity"
      ],
      "metadata": {
        "id": "V229Dec71562"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for compute the NPV\n",
        "def NPV_score(y_true,y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.shape != (2, 2):\n",
        "        raise ValueError('The problem must be binary for NPV')\n",
        "    NPV = cm[0][0] / (cm[0][0] + cm[1][0])\n",
        "    return NPV\n"
      ],
      "metadata": {
        "id": "Z6GE8pJk8vnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for display the ROC curve and the confusion matrix\n",
        "def plots(cm,fpr,tpr,roc_auc,classes,y_true,y_pred, save_plots):\n",
        "      # Create a new figure and axes for the confusion matrix and ROC curve plots\n",
        "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    im = ax1.imshow(cm, interpolation='nearest', cmap=plt.cm.RdYlBu)\n",
        "    ax1.set_title('Confusion matrix')\n",
        "    ax1.set_xticks(np.arange(len(classes)))\n",
        "    ax1.set_yticks(np.arange(len(classes)))\n",
        "    ax1.set_xticklabels(classes, rotation=45)\n",
        "    ax1.set_yticklabels(classes)\n",
        "    ax1.set_ylabel('True label')\n",
        "    ax1.set_xlabel('Predicted label')\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax1.text(j, i, format(cm[i, j], '.2f'),\n",
        "                 horizontalalignment='center',\n",
        "                 color='black' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "    # Plot ROC curve\n",
        "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='ROC Curve')\n",
        "    roc_display.plot(ax=ax2)\n",
        "    ax2.set_title('Receiver Operating Characteristic')\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "        # Save plot if requested\n",
        "    if save_plots:\n",
        "        save_path = os.path.join('.', \"roc_cm_plot.png\")\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HVytgSFRNKx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(y_true,y_pred,y_proba,classes, save_plots = False):\n",
        "    # this is a function that compute the evaluation metrics. the metrics chosen are:\n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "\n",
        "    #accuracy: function accuracy_score of the sklearn.metrics library\n",
        "    accuracy = accuracy_score(y_true,y_pred)\n",
        "\n",
        "    #balanced accuracy: function balanced_accuracy_score of the sklearn.metrics library\n",
        "    balanced_accuracy = balanced_accuracy_score(y_true,y_pred)\n",
        "\n",
        "    #precision: function precision_score of the sklearn.metrics library\n",
        "    precision = precision_score(y_true,y_pred)\n",
        "\n",
        "    #recall: function recall_score of the sklearn.metrics library\n",
        "    recall = recall_score(y_true,y_pred, zero_division=1) #zero_division=1 for not true positives\n",
        "\n",
        "    #specificity: function specificity_score built by us\n",
        "    specificity = specificity_score(y_true,y_pred)\n",
        "\n",
        "    #NPV: function negative_predicted_value built by us\n",
        "    NPV = NPV_score(y_true,y_pred)\n",
        "\n",
        "    #f1_score: function f1_score of the sklearn.metrics library\n",
        "    f1 = f1_score(y_true,y_pred)\n",
        "\n",
        "    #ROC: function roc_curve of the sklearn.metrics library to compute it and the function RocCurveDisplay for the plot\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "\n",
        "    #AUC: function auc of the sklearn.metrics library\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plots(cm,fpr,tpr,roc_auc,classes,y_true,y_pred, save_plots)\n",
        "\n",
        "    # Return evaluation metrics\n",
        "    return cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vaQevzmokzbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_dict(y_true, y_pred, y_proba, classes):\n",
        "    # Initialize a dictionary to store the metrics for each fold\n",
        "    metrics_dict = {'cm': [], 'accuracy': [], 'balanced_accuracy': [], 'precision': [], 'recall': [], 'specificity': [], 'NPV': [], 'f1': [], 'roc_auc': [], 'thresholds': []}\n",
        "\n",
        "    # Compute the evaluation metrics for each fold and store them in the metrics_dict\n",
        "    for i in range(len(y_true)):\n",
        "        cm = confusion_matrix(y_true[i], y_pred[i])\n",
        "        accuracy = accuracy_score(y_true[i], y_pred[i])\n",
        "        balanced_accuracy = balanced_accuracy_score(y_true[i], y_pred[i])\n",
        "        precision = precision_score(y_true[i], y_pred[i])\n",
        "        recall = recall_score(y_true[i], y_pred[i])\n",
        "        specificity = specificity_score(y_true[i], y_pred[i])\n",
        "        NPV = NPV_score(y_true[i], y_pred[i])\n",
        "        f1 = f1_score(y_true[i], y_pred[i])\n",
        "        fpr, tpr, thresholds = roc_curve(y_true[i], y_proba[i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        metrics_dict['cm'].append(cm)\n",
        "        metrics_dict['accuracy'].append(accuracy)\n",
        "        metrics_dict['balanced_accuracy'].append(balanced_accuracy)\n",
        "        metrics_dict['precision'].append(precision)\n",
        "        metrics_dict['recall'].append(recall)\n",
        "        metrics_dict['specificity'].append(specificity)\n",
        "        metrics_dict['NPV'].append(NPV)\n",
        "        metrics_dict['f1'].append(f1)\n",
        "        metrics_dict['roc_auc'].append(roc_auc)\n",
        "        metrics_dict['thresholds'].append(thresholds)\n",
        "\n",
        "        # Plot the confusion matrix and ROC curve for each fold\n",
        "        plots(cm, fpr, tpr, roc_auc, classes, y_true[i], y_pred[i])\n",
        "\n",
        "    # Return the metrics dictionary containing all metrics for each fold\n",
        "    return metrics_dict\n"
      ],
      "metadata": {
        "id": "svJhkVvwOc6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}